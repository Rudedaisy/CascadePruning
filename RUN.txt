# Re-training
CUDA_VISIBLE_DEVICES=2,3 python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 run.py --model alexnet --dataset ImageNet --batch 512 --epochs 100 --spar-str 2e-4 --scheduler=coswm --data-dir= <INSERT DATASET PATH HERE>

# Pruning and fine-tuning
CUDA_VISIBLE_DEVICES=2,3 python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 run.py --model alexnet --dataset ImageNet --batch 512 --epochs 99 --epochs-ft 30 --scheduler=coswm --ckpt-dir <INSERTTRAINED PATH HERE> --prune --q 0.75


## EfficientNet-b1 training commands (Original TOP1 test accuracy is 79.1%)

CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 run.py --model efficientnet --dataset ImageNet --batch 512 --epochs 120 --epochs-ft 50 --spar-str 5e-4 --lr 1e-4 --lr-ft 1e-4 --reg-ft 1e-4 --scheduler=coswm

CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 run.py --model efficientnet --dataset ImageNet --batch 512 --epochs 117 --epochs-ft 50 --spar-str 5e-4 --lr 1e-4 --lr-ft 1e-4 --reg-ft 1e-4 --scheduler=coswm --ckpt-dir <INSERT TRAINED PATH HERE> --prune --prune-type cascade --q 0.75

## Cambricon-S AlexNet training commands

python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 run.py --model alexnet --dataset ImageNet --batch 512 --epochs 120 --epochs-ft 50 --spar-str 1e-4 --lr 1e-4 --lr-ft 1e-4 --reg-ft 1e-4 --scheduler=coswm --spar-reg v1

python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 run.py --model alexnet --dataset ImageNet --batch 512 --epochs 120 --epochs-ft 50 --spar-str 1e-4 --lr 1e-4 --lr-ft 1e-4 --reg-ft 1e-4 --scheduler=coswm --spar-reg v1 --ckpt-dir <INSERT TRAINED PATH HERE> --prune --prune-type cs --q 0.75
